{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "HxccwNcIUuVV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "from collections import OrderedDict\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms , datasets\n",
        "from torchsummary import summary\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfVEBTd4fnOb",
        "outputId": "1cd4c218-aeb0-4845-ebc2-adc0edc31546"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Z2idpmtCf0sx"
      },
      "outputs": [],
      "source": [
        "# this class acts as interface to implement BasicConvBlock\n",
        "\n",
        "class LambdaLayer(nn.Module):\n",
        "  def __init__(self, lambd):\n",
        "    super(LambdaLayer, self).__init__()\n",
        "    self.lambd = lambd\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.lambd(x)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this ResNet implementation we will implement a 56 layer architecture. The total number of stack layers are 6*n+2 ( here n=9 ) thus 56 layers. \\\n",
        "The architecture goes as: \\\n",
        "1. simple Convolutions layers with filter 7x7 and Maxpooling layers reducing the dimensions while increaseing the number of channels. \\\n",
        "2. Then 3x3 convolution are implemented as the ConvBlock containing a stack of 2 layers to which the skip connections will be added i.e. \\\n",
        "a. 3x3 conv + batchnorm + relu \\\n",
        "b. 3x3 conv2d + batchnorm \\\n",
        "c. Then the skip connection (identity) is added to the last output ofcourse the dimensions are matched \\\n",
        "This small architecture is the Residual Block\n",
        "3. such n block form the first part with feature map size preserved as 32\n",
        "4. this feature map size is subsampled to 16 with stride of 2\n",
        "5. then 2 times the process is repeated with number of filters as 32, and 64 repestively\n"
      ],
      "metadata": {
        "id": "JJOqlMmyYZVS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicConvBlock(nn.Module):\n",
        "  ''' The BasicConvBlock takes an input with in_channels, applies sone blocks of convolutional layers\n",
        "  to reduce it to out_channels and sum it up to the original input\n",
        "  If their sizes mismatch then the input goes into as identity\n",
        "\n",
        "  Basically the basic CovnBlock will implement the regular basic Conv Block +\n",
        "  the shortcut block that doens the dimension matching job ( optionA or B with reference to research paper) when dimension changes between 2 blocks\n",
        "  '''\n",
        "\n",
        "  def __init__( self, in_channels, out_channels, stride=1 , option ='A'):\n",
        "    super(BasicConvBlock, self).__init__()\n",
        "\n",
        "    self.features = nn.Sequential( OrderedDict([\n",
        "        ('conv1', nn.Conv2d( in_channels, out_channels, kernel_size=3, stride= stride, padding = 1, bias = False )),\n",
        "        ('bn1', nn.BatchNorm2d(out_channels)),\n",
        "        ('act1', nn.ReLU(inplace = True)),\n",
        "        ('conv2', nn.Conv2d( out_channels, out_channels, kernel_size =3, stride = 1, padding = 1, bias = False)),\n",
        "        ('bn2', nn.BatchNorm2d(out_channels))\n",
        "    ]))\n",
        "\n",
        "    self.shortcut = nn.Sequential()\n",
        "\n",
        "    '''When input and output spatial dimension dont match we have 2 opions , with stride:\n",
        "      - A Use identity shortcuts with zero padding to increase channel dimension.\n",
        "      - B Use 1x1 convolution to increase channels dimensions ( projection shortcut)\n",
        "    '''\n",
        "\n",
        "    if stride != 1 or in_channels != out_channels:\n",
        "      if option ==\"A\":\n",
        "        # use identity shortcuts with zero padding to increase channel dimension\n",
        "        pad_to_add= out_channels//4\n",
        "        ''' ::2 is doing the job of strid =2\n",
        "        F.pad apply padding to ( W,H, C,N)\n",
        "\n",
        "        The padding lengths are specified in reverse order of the dimensions,\n",
        "        F.pad( x[:, :, ::2, ::2], (0,0, 0,0, pad,pad, 0,0))\n",
        "\n",
        "        [ width_beginning, widht_end, height_beginning, height_end, channel_beginning, channel_end, batch_beginning, batch_end]\n",
        "        '''\n",
        "        self.shortcut = LambdaLayer( lambda x: F.pad(x[:, :, ::2, ::2], (0,0, 0,0, pad_to_add,pad_to_add, 0,0)) )\n",
        "\n",
        "      if option ==\"B\":\n",
        "        self.shortcut = nn.Sequential( OrderedDict([\n",
        "            ( 's_conv1', nn.Conv2d( in_channels, 2*out_channels, kernel_size = 1, stride = stride, padding = 0,  bias = False)),\n",
        "            ( 's_bn1', nn.BatchNorm2d(2*out_channels))\n",
        "        ]))\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.features(x)\n",
        "    out += self.shortcut(x) # adding the shortcut layer to the current layer output ( the residual is added here )\n",
        "    out = F.relu(out) # applying the activation functions\n",
        "    return out\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cNjeecAGX6iH"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNet(nn.Module):\n",
        "  '''ResNet-56 Architecture for CIFAR 10 Dataset of shape 32x32x3\n",
        "  '''\n",
        "  def __init__(self, block_type, num_blocks):\n",
        "    super(ResNet,self).__init__()\n",
        "\n",
        "    self.in_channels = 16\n",
        "\n",
        "    self.conv0 = nn.Conv2d( 3, 16, kernel_size = 3, stride = 1, padding = 1, bias = False) # first simple conv layer with # of filter = 16\n",
        "    self.bn0 = nn.BatchNorm2d(16) # batchnormalizaton\n",
        "    # num_blocks = value of n , here it is 9 and 6n+2 =56\n",
        "    # _build_layer will implement the 2n layers for each block before pooling into lesser dimension with strides  with n residual connections\n",
        "    self.block1 = self.__build_layer( block_type, 16 , num_blocks[0], starting_stride = 1) #\n",
        "    self.block2 = self.__build_layer( block_type, 32 , num_blocks[1], starting_stride = 2)\n",
        "    self.block3 = self.__build_layer( block_type, 64 , num_blocks[2], starting_stride = 2)\n",
        "\n",
        "    self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
        "    self.linear = nn.Linear(64,10)\n",
        "\n",
        "    # building the n layers for each block in resnet\n",
        "  def __build_layer( self, block_type, out_channels, num_blocks, starting_stride):\n",
        "\n",
        "    # stride for the first layer is 2 as to decrease the mapping output size to compensate the calculation complexity bcoz of increase the number of channels\n",
        "    stride_list_for_current_block = [starting_stride] + [1]*(num_blocks-1)\n",
        "    ''' Above line will generate an array whose first element is starting_stride and it wil have (num_blocks-1) more elements each of value 1'''\n",
        "\n",
        "    layers = []\n",
        "\n",
        "    for stride in stride_list_for_current_block:\n",
        "      layers.append( block_type( self.in_channels, out_channels, stride)) # appending all the convolution layers for a block\n",
        "      self.in_channels = out_channels\n",
        "\n",
        "    return nn.Sequential(*layers) # return all the convolution layers stacked in sequence\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = F.relu( self.bn0(self.conv0(x))) # need to understand this\n",
        "    out = self.block1(out)\n",
        "    out = self.block2(out)\n",
        "    out = self.block3(out)\n",
        "    out = self.avgpool(out)\n",
        "    out = out.view(out.size(0), -1) # flattening the avpool layer which of size batch_size x channel x height x width i.e. batch_size x 64x1x1\n",
        "    out = self.linear(out)\n",
        "    return out\n"
      ],
      "metadata": {
        "id": "-8bqybPn4BRs"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ResNet56():\n",
        "  return ResNet(block_type = BasicConvBlock, num_blocks= [ 9,9,9])"
      ],
      "metadata": {
        "id": "EtGyCVhg-8-3"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ResNet56()\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "summary(model, (3, 32, 32))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lYwecBnNFs4h",
        "outputId": "81a22e57-fd54-4892-8e39-bed5f75e310a"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 16, 32, 32]             432\n",
            "       BatchNorm2d-2           [-1, 16, 32, 32]              32\n",
            "            Conv2d-3           [-1, 16, 32, 32]           2,304\n",
            "       BatchNorm2d-4           [-1, 16, 32, 32]              32\n",
            "              ReLU-5           [-1, 16, 32, 32]               0\n",
            "            Conv2d-6           [-1, 16, 32, 32]           2,304\n",
            "       BatchNorm2d-7           [-1, 16, 32, 32]              32\n",
            "    BasicConvBlock-8           [-1, 16, 32, 32]               0\n",
            "            Conv2d-9           [-1, 16, 32, 32]           2,304\n",
            "      BatchNorm2d-10           [-1, 16, 32, 32]              32\n",
            "             ReLU-11           [-1, 16, 32, 32]               0\n",
            "           Conv2d-12           [-1, 16, 32, 32]           2,304\n",
            "      BatchNorm2d-13           [-1, 16, 32, 32]              32\n",
            "   BasicConvBlock-14           [-1, 16, 32, 32]               0\n",
            "           Conv2d-15           [-1, 16, 32, 32]           2,304\n",
            "      BatchNorm2d-16           [-1, 16, 32, 32]              32\n",
            "             ReLU-17           [-1, 16, 32, 32]               0\n",
            "           Conv2d-18           [-1, 16, 32, 32]           2,304\n",
            "      BatchNorm2d-19           [-1, 16, 32, 32]              32\n",
            "   BasicConvBlock-20           [-1, 16, 32, 32]               0\n",
            "           Conv2d-21           [-1, 16, 32, 32]           2,304\n",
            "      BatchNorm2d-22           [-1, 16, 32, 32]              32\n",
            "             ReLU-23           [-1, 16, 32, 32]               0\n",
            "           Conv2d-24           [-1, 16, 32, 32]           2,304\n",
            "      BatchNorm2d-25           [-1, 16, 32, 32]              32\n",
            "   BasicConvBlock-26           [-1, 16, 32, 32]               0\n",
            "           Conv2d-27           [-1, 16, 32, 32]           2,304\n",
            "      BatchNorm2d-28           [-1, 16, 32, 32]              32\n",
            "             ReLU-29           [-1, 16, 32, 32]               0\n",
            "           Conv2d-30           [-1, 16, 32, 32]           2,304\n",
            "      BatchNorm2d-31           [-1, 16, 32, 32]              32\n",
            "   BasicConvBlock-32           [-1, 16, 32, 32]               0\n",
            "           Conv2d-33           [-1, 16, 32, 32]           2,304\n",
            "      BatchNorm2d-34           [-1, 16, 32, 32]              32\n",
            "             ReLU-35           [-1, 16, 32, 32]               0\n",
            "           Conv2d-36           [-1, 16, 32, 32]           2,304\n",
            "      BatchNorm2d-37           [-1, 16, 32, 32]              32\n",
            "   BasicConvBlock-38           [-1, 16, 32, 32]               0\n",
            "           Conv2d-39           [-1, 16, 32, 32]           2,304\n",
            "      BatchNorm2d-40           [-1, 16, 32, 32]              32\n",
            "             ReLU-41           [-1, 16, 32, 32]               0\n",
            "           Conv2d-42           [-1, 16, 32, 32]           2,304\n",
            "      BatchNorm2d-43           [-1, 16, 32, 32]              32\n",
            "   BasicConvBlock-44           [-1, 16, 32, 32]               0\n",
            "           Conv2d-45           [-1, 16, 32, 32]           2,304\n",
            "      BatchNorm2d-46           [-1, 16, 32, 32]              32\n",
            "             ReLU-47           [-1, 16, 32, 32]               0\n",
            "           Conv2d-48           [-1, 16, 32, 32]           2,304\n",
            "      BatchNorm2d-49           [-1, 16, 32, 32]              32\n",
            "   BasicConvBlock-50           [-1, 16, 32, 32]               0\n",
            "           Conv2d-51           [-1, 16, 32, 32]           2,304\n",
            "      BatchNorm2d-52           [-1, 16, 32, 32]              32\n",
            "             ReLU-53           [-1, 16, 32, 32]               0\n",
            "           Conv2d-54           [-1, 16, 32, 32]           2,304\n",
            "      BatchNorm2d-55           [-1, 16, 32, 32]              32\n",
            "   BasicConvBlock-56           [-1, 16, 32, 32]               0\n",
            "           Conv2d-57           [-1, 32, 16, 16]           4,608\n",
            "      BatchNorm2d-58           [-1, 32, 16, 16]              64\n",
            "             ReLU-59           [-1, 32, 16, 16]               0\n",
            "           Conv2d-60           [-1, 32, 16, 16]           9,216\n",
            "      BatchNorm2d-61           [-1, 32, 16, 16]              64\n",
            "      LambdaLayer-62           [-1, 32, 16, 16]               0\n",
            "   BasicConvBlock-63           [-1, 32, 16, 16]               0\n",
            "           Conv2d-64           [-1, 32, 16, 16]           9,216\n",
            "      BatchNorm2d-65           [-1, 32, 16, 16]              64\n",
            "             ReLU-66           [-1, 32, 16, 16]               0\n",
            "           Conv2d-67           [-1, 32, 16, 16]           9,216\n",
            "      BatchNorm2d-68           [-1, 32, 16, 16]              64\n",
            "   BasicConvBlock-69           [-1, 32, 16, 16]               0\n",
            "           Conv2d-70           [-1, 32, 16, 16]           9,216\n",
            "      BatchNorm2d-71           [-1, 32, 16, 16]              64\n",
            "             ReLU-72           [-1, 32, 16, 16]               0\n",
            "           Conv2d-73           [-1, 32, 16, 16]           9,216\n",
            "      BatchNorm2d-74           [-1, 32, 16, 16]              64\n",
            "   BasicConvBlock-75           [-1, 32, 16, 16]               0\n",
            "           Conv2d-76           [-1, 32, 16, 16]           9,216\n",
            "      BatchNorm2d-77           [-1, 32, 16, 16]              64\n",
            "             ReLU-78           [-1, 32, 16, 16]               0\n",
            "           Conv2d-79           [-1, 32, 16, 16]           9,216\n",
            "      BatchNorm2d-80           [-1, 32, 16, 16]              64\n",
            "   BasicConvBlock-81           [-1, 32, 16, 16]               0\n",
            "           Conv2d-82           [-1, 32, 16, 16]           9,216\n",
            "      BatchNorm2d-83           [-1, 32, 16, 16]              64\n",
            "             ReLU-84           [-1, 32, 16, 16]               0\n",
            "           Conv2d-85           [-1, 32, 16, 16]           9,216\n",
            "      BatchNorm2d-86           [-1, 32, 16, 16]              64\n",
            "   BasicConvBlock-87           [-1, 32, 16, 16]               0\n",
            "           Conv2d-88           [-1, 32, 16, 16]           9,216\n",
            "      BatchNorm2d-89           [-1, 32, 16, 16]              64\n",
            "             ReLU-90           [-1, 32, 16, 16]               0\n",
            "           Conv2d-91           [-1, 32, 16, 16]           9,216\n",
            "      BatchNorm2d-92           [-1, 32, 16, 16]              64\n",
            "   BasicConvBlock-93           [-1, 32, 16, 16]               0\n",
            "           Conv2d-94           [-1, 32, 16, 16]           9,216\n",
            "      BatchNorm2d-95           [-1, 32, 16, 16]              64\n",
            "             ReLU-96           [-1, 32, 16, 16]               0\n",
            "           Conv2d-97           [-1, 32, 16, 16]           9,216\n",
            "      BatchNorm2d-98           [-1, 32, 16, 16]              64\n",
            "   BasicConvBlock-99           [-1, 32, 16, 16]               0\n",
            "          Conv2d-100           [-1, 32, 16, 16]           9,216\n",
            "     BatchNorm2d-101           [-1, 32, 16, 16]              64\n",
            "            ReLU-102           [-1, 32, 16, 16]               0\n",
            "          Conv2d-103           [-1, 32, 16, 16]           9,216\n",
            "     BatchNorm2d-104           [-1, 32, 16, 16]              64\n",
            "  BasicConvBlock-105           [-1, 32, 16, 16]               0\n",
            "          Conv2d-106           [-1, 32, 16, 16]           9,216\n",
            "     BatchNorm2d-107           [-1, 32, 16, 16]              64\n",
            "            ReLU-108           [-1, 32, 16, 16]               0\n",
            "          Conv2d-109           [-1, 32, 16, 16]           9,216\n",
            "     BatchNorm2d-110           [-1, 32, 16, 16]              64\n",
            "  BasicConvBlock-111           [-1, 32, 16, 16]               0\n",
            "          Conv2d-112             [-1, 64, 8, 8]          18,432\n",
            "     BatchNorm2d-113             [-1, 64, 8, 8]             128\n",
            "            ReLU-114             [-1, 64, 8, 8]               0\n",
            "          Conv2d-115             [-1, 64, 8, 8]          36,864\n",
            "     BatchNorm2d-116             [-1, 64, 8, 8]             128\n",
            "     LambdaLayer-117             [-1, 64, 8, 8]               0\n",
            "  BasicConvBlock-118             [-1, 64, 8, 8]               0\n",
            "          Conv2d-119             [-1, 64, 8, 8]          36,864\n",
            "     BatchNorm2d-120             [-1, 64, 8, 8]             128\n",
            "            ReLU-121             [-1, 64, 8, 8]               0\n",
            "          Conv2d-122             [-1, 64, 8, 8]          36,864\n",
            "     BatchNorm2d-123             [-1, 64, 8, 8]             128\n",
            "  BasicConvBlock-124             [-1, 64, 8, 8]               0\n",
            "          Conv2d-125             [-1, 64, 8, 8]          36,864\n",
            "     BatchNorm2d-126             [-1, 64, 8, 8]             128\n",
            "            ReLU-127             [-1, 64, 8, 8]               0\n",
            "          Conv2d-128             [-1, 64, 8, 8]          36,864\n",
            "     BatchNorm2d-129             [-1, 64, 8, 8]             128\n",
            "  BasicConvBlock-130             [-1, 64, 8, 8]               0\n",
            "          Conv2d-131             [-1, 64, 8, 8]          36,864\n",
            "     BatchNorm2d-132             [-1, 64, 8, 8]             128\n",
            "            ReLU-133             [-1, 64, 8, 8]               0\n",
            "          Conv2d-134             [-1, 64, 8, 8]          36,864\n",
            "     BatchNorm2d-135             [-1, 64, 8, 8]             128\n",
            "  BasicConvBlock-136             [-1, 64, 8, 8]               0\n",
            "          Conv2d-137             [-1, 64, 8, 8]          36,864\n",
            "     BatchNorm2d-138             [-1, 64, 8, 8]             128\n",
            "            ReLU-139             [-1, 64, 8, 8]               0\n",
            "          Conv2d-140             [-1, 64, 8, 8]          36,864\n",
            "     BatchNorm2d-141             [-1, 64, 8, 8]             128\n",
            "  BasicConvBlock-142             [-1, 64, 8, 8]               0\n",
            "          Conv2d-143             [-1, 64, 8, 8]          36,864\n",
            "     BatchNorm2d-144             [-1, 64, 8, 8]             128\n",
            "            ReLU-145             [-1, 64, 8, 8]               0\n",
            "          Conv2d-146             [-1, 64, 8, 8]          36,864\n",
            "     BatchNorm2d-147             [-1, 64, 8, 8]             128\n",
            "  BasicConvBlock-148             [-1, 64, 8, 8]               0\n",
            "          Conv2d-149             [-1, 64, 8, 8]          36,864\n",
            "     BatchNorm2d-150             [-1, 64, 8, 8]             128\n",
            "            ReLU-151             [-1, 64, 8, 8]               0\n",
            "          Conv2d-152             [-1, 64, 8, 8]          36,864\n",
            "     BatchNorm2d-153             [-1, 64, 8, 8]             128\n",
            "  BasicConvBlock-154             [-1, 64, 8, 8]               0\n",
            "          Conv2d-155             [-1, 64, 8, 8]          36,864\n",
            "     BatchNorm2d-156             [-1, 64, 8, 8]             128\n",
            "            ReLU-157             [-1, 64, 8, 8]               0\n",
            "          Conv2d-158             [-1, 64, 8, 8]          36,864\n",
            "     BatchNorm2d-159             [-1, 64, 8, 8]             128\n",
            "  BasicConvBlock-160             [-1, 64, 8, 8]               0\n",
            "          Conv2d-161             [-1, 64, 8, 8]          36,864\n",
            "     BatchNorm2d-162             [-1, 64, 8, 8]             128\n",
            "            ReLU-163             [-1, 64, 8, 8]               0\n",
            "          Conv2d-164             [-1, 64, 8, 8]          36,864\n",
            "     BatchNorm2d-165             [-1, 64, 8, 8]             128\n",
            "  BasicConvBlock-166             [-1, 64, 8, 8]               0\n",
            "AdaptiveAvgPool2d-167             [-1, 64, 1, 1]               0\n",
            "          Linear-168                   [-1, 10]             650\n",
            "================================================================\n",
            "Total params: 853,018\n",
            "Trainable params: 853,018\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 12.16\n",
            "Params size (MB): 3.25\n",
            "Estimated Total Size (MB): 15.42\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading the CIFAR-10 Dataset"
      ],
      "metadata": {
        "id": "kmV2nt95Jzzj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Dataloader_cifar():\n",
        "\n",
        "  transform = transforms.Compose([ transforms.ToTensor(), transforms.Normalize( mean = [0.5], std= [0.5])])\n",
        "\n",
        "  root_input_dataset_dir =  \"/content/drive/My Drive/input_dataset\"\n",
        "  train_dataset = datasets.CIFAR10( root_input_dataset_dir, download =True, train= True, transform = transform )\n",
        "  test_dataset = datasets.CIFAR10( root_input_dataset_dir, download =True, train = False, transform = transform )\n",
        "\n",
        "  train_dataset, val_dataset = random_split( train_dataset, [45000, 5000])\n",
        "\n",
        "  print(f\"Image shape of a random sample image : {train_dataset[0][0].numpy().shape}\", end = \"\\n\\n\")\n",
        "  print(f\"Training Set Size : {len(train_dataset)}\", end = \"\\n\\n\")\n",
        "  print(f\"Validation Set Size : {len(val_dataset)}\", end = \"\\n\\n\")\n",
        "  print(f\"Test Set Size : {len(test_dataset)}\")\n",
        "\n",
        "  batch_size =32\n",
        "\n",
        "  train_loader = DataLoader( train_dataset, batch_size= batch_size, shuffle =True)\n",
        "  val_loader = DataLoader( val_dataset, batch_size = batch_size, shuffle = True)\n",
        "  test_loader = DataLoader(test_dataset, batch_size = 1000, shuffle =True)\n",
        "\n",
        "  return train_loader, val_loader, test_loader\n"
      ],
      "metadata": {
        "id": "3ZlVFW7RF7qy"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader , val_loader , test_loader = Dataloader_cifar()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FzGaiMTvL_CW",
        "outputId": "1ba87d62-a34d-45e3-e54b-a8b131d5f932"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Image shape of a random sample image : (3, 32, 32)\n",
            "\n",
            "Training Set Size : 45000\n",
            "\n",
            "Validation Set Size : 5000\n",
            "\n",
            "Test Set Size : 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD( model.parameters(), lr = 0.001)"
      ],
      "metadata": {
        "id": "0KF2xyLbM6gM"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model():\n",
        "\n",
        "  EPOCHS = 15\n",
        "  train_examples_num = len(train_loader.dataset)\n",
        "  val_examples_num = len(val_loader.dataset)\n",
        "  train_costs, val_costs = [], []\n",
        "\n",
        "  # training phase\n",
        "  for epoch in range(EPOCHS):\n",
        "\n",
        "    train_running_loss= 0\n",
        "    correct_train = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "      inputs, labels = inputs.to(device) , labels.to(device)\n",
        "\n",
        "      ''' for every mini-batch during the training phase we typically want to explicitly set the gradients to zero before starting to do backpropagation '''\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      prediction = model(inputs)\n",
        "      loss= criterion(prediction, labels)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "\n",
        "\n",
        "      #find the maximum along the row use dim =1 to torch.max()\n",
        "      _, predicted_output= torch.max(prediction.data, 1)\n",
        "\n",
        "      # update the running correct\n",
        "      correct_train += (predicted_output ==labels).float().sum().item()\n",
        "\n",
        "      ''' Compute batch loss\n",
        "      multiply each average batch loss with batch length\n",
        "      the batch-length is input.size(0) which gives the number total images in each batch.\n",
        "      Essentially I am un avereaging the previously calculated Loss '''\n",
        "      train_running_loss += loss.item()*inputs.size(0)\n",
        "\n",
        "    train_epoch_loss = train_running_loss/ train_examples_num\n",
        "    train_costs.append(train_epoch_loss)\n",
        "    train_epoch_accuracy = correct_train/ train_examples_num\n",
        "\n",
        "    val_running_loss= 0\n",
        "    correct_val = 0\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      for inputs_labels in val_loader:\n",
        "        inputs, labels = inputs_labels[0].to(device) , inputs_labels[1].to(device)\n",
        "\n",
        "        prediction = model(inputs)\n",
        "        loss= criterion(prediction, labels)\n",
        "\n",
        "        _, predicted_output= torch.max(prediction.data, 1)\n",
        "        correct_val += (predicted_output ==labels).float().sum().item()\n",
        "\n",
        "        val_running_loss += loss.item()*inputs.size(0)\n",
        "\n",
        "    val_epoch_loss = val_running_loss/ val_examples_num\n",
        "    val_costs.append(val_epoch_loss)\n",
        "    val_epoch_accuracy = correct_val/ val_examples_num\n",
        "\n",
        "    print(f\"Epoch {epoch+1/EPOCHS}: train-loss = {train_epoch_loss:0.6f} | train-acc = {train_epoch_accuracy:0.6f} | val-loss = {val_epoch_loss:0.6f} | val-acc = {val_epoch_accuracy:0.6f}\")\n",
        "\n",
        "    torch.save(model.state_dict(),f\"/content/drive/MyDrive/ResNet56_model/checkpoint_gpu_{epoch+1}\")\n",
        "\n",
        "  torch.save(model.state_dict(),f\"/content/drive/MyDrive/ResNet56_model/final_gpu\")\n",
        "\n",
        "  return train_costs, val_costs\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ylTjUOXTNEKN"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_costs , val_costs = train_model()"
      ],
      "metadata": {
        "id": "HY9IaGy8VngN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ResNet56()\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/ResNet56_model/final_gpu'))\n"
      ],
      "metadata": {
        "id": "6RPBUV4KVvC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing the model"
      ],
      "metadata": {
        "id": "B_Omo1cyV3Pn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_samples_num = len(test_loader)\n",
        "correct = 0\n",
        "\n",
        "model.eval().cuda()\n",
        "\n",
        "with torch.no_grad():\n",
        "  for inputs, labels in test_loader:\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "    predictions = model(inputs)\n",
        "    _, predicted_output = torch.max(predictions.data, 1)\n",
        "    correct += (predicted_output == labels).float().sum().item()\n",
        "\n",
        "test_accuracy = correct/test_samples_num\n",
        "print(f\"Test Accuracy : {test_accuracy}\")"
      ],
      "metadata": {
        "id": "h0mvxWIJV2dO"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}